{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning Gemma2 2B model on Roadrunner with JAX, Flax.\n",
    "\n",
    "We have adopted the Gemma notebook from Google Deepmind to use HuggingFace's libraries, added support for doing **model parallel training** and simplified the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "def import_local_module(module_path: str):\n",
    "    sys.path.append('')\n",
    "    module = importlib.import_module(module_path)\n",
    "    return importlib.reload(module)\n",
    "\n",
    "# Import felafax libraries\n",
    "setup = import_local_module(\"trainer_engine.setup\")\n",
    "setup.setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().update(setup.setup_imports())\n",
    "\n",
    "utils = import_local_module(\"trainer_engine.utils\")\n",
    "training_pipeline = import_local_module(\"trainer_engine.training_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Input your HF username, token and download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: Please provide your HUGGINGFACE_USERNAME:  felarof01\n",
      "INPUT: Please provide your HUGGINGFACE_TOKEN:  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\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace username and token to use when downloading.\n",
    "MODEL_NAME=\"felafax/gemma-2-2b-it-JAX\"\n",
    "HUGGINGFACE_USERNAME = input(\"INPUT: Please provide your HUGGINGFACE_USERNAME: \")\n",
    "HUGGINGFACE_TOKEN = input(\"INPUT: Please provide your HUGGINGFACE_TOKEN: \")\n",
    "\n",
    "model_name=MODEL_NAME\n",
    "hugging_face_token=HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "ckpt_path = snapshot_download(repo_id=MODEL_NAME, token=HUGGINGFACE_TOKEN)\n",
    "vocab_path = os.path.join(ckpt_path, 'tokenizer.model')\n",
    "\n",
    "# Load parameters.\n",
    "params = {\"params\": params_lib.load_and_format_params(os.path.join(ckpt_path, 'gemma2-2b-it'))['transformer']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: prepare the dataset\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=1, max_length=32, max_examples=None):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    def _tokenize(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length+1)\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        tokenized['target_mask'] = [input_id[:-1] for input_id in tokenized['attention_mask']]\n",
    "        return {\n",
    "            'input_tokens': tokenized['input_ids'],\n",
    "            'target_mask': tokenized['target_mask']\n",
    "        }\n",
    "\n",
    "    def _custom_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Collates batch items and converts PyTorch tensors to JAX arrays.\n",
    "        Applies default_data_collator, then converts tensors to JAX format.\n",
    "        \"\"\"\n",
    "        collated = default_data_collator(batch)\n",
    "        jax_batch = {}\n",
    "        for key, value in collated.items():\n",
    "            jax_batch[key] = jnp.array(value.numpy()) if isinstance(value, torch.Tensor) else value\n",
    "        \n",
    "        return jax_batch\n",
    "\n",
    "    # Load and preprocess the dataset\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(max_examples))\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    for split in ['train', 'test']:\n",
    "        ds[split] = ds[split].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size, collate_fn=_custom_collate_fn)\n",
    "    train_dataloader = torch.utils.data.DataLoader(ds['train'], **dataloader_args)\n",
    "    test_dataloader = torch.utils.data.DataLoader(ds['test'], **dataloader_args)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to test dataset pipeline\n",
    "# def test_dataset_pipeline(tokenizer):\n",
    "#     \"\"\"Print shapes of first batch to verify dataset pipeline.\"\"\"\n",
    "#     train_loader, _ = get_dataset(tokenizer=tokenizer, batch_size=2, max_length=64)\n",
    "#     batch = next(iter(train_loader))\n",
    "#     print(\"Input tokens shape:\", batch['input_tokens'].shape)\n",
    "#     print(\"Target mask shape:\", batch['target_mask'].shape)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     token=HUGGINGFACE_TOKEN\n",
    "# )\n",
    "# test_dataset_pipeline(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the model by configuring the hyperparameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  batch_size: int\n",
    "  eval_every_n: int\n",
    "\n",
    "  # Maximum number of training steps (if None, train for full num_epochs)\n",
    "  max_steps: int | None = None\n",
    "  # Limit on number of dataset examples for faster training (if None, use full dataset)\n",
    "  dataset_size_limit: int | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=20,\n",
    "                              batch_size=1,\n",
    "                              max_steps=10, \n",
    "                              dataset_size_limit=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config.\n",
    "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=30)\n",
    "model = transformer_lib.Transformer(config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "optimizer = optax.sgd(training_cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device mesh for distributing the model across TPU cores and do model parallel training.\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4, 1))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model', 'replica'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e03ccc308464571be672edca93868d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4d87b914c545138f206e80044138c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Train Loss: 3.5580\n",
      "Step 2, Train Loss: 3.2197\n",
      "Step 3, Train Loss: 2.9345\n",
      "Step 4, Train Loss: 2.6793\n",
      "Step 5, Train Loss: 2.4685\n",
      "Step 6, Train Loss: 2.2560\n",
      "Step 7, Train Loss: 2.0905\n",
      "Step 8, Train Loss: 1.9558\n",
      "Step 9, Train Loss: 1.8249\n",
      "Step 10, Train Loss: 1.7283\n",
      "Training complete. Average loss: 2.4715\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_dataset(tokenizer=tokenizer, max_examples=training_cfg.dataset_size_limit)\n",
    "\n",
    "new_state = training_pipeline.train_loop(model=model,\n",
    "                    params=params,\n",
    "                    optimizer=optimizer,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    tokenizer=tokenizer,\n",
    "                    training_cfg=training_cfg, \n",
    "                    mesh = mesh)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
