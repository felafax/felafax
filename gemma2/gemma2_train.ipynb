{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning Gemma2 2B model on Roadrunner with JAX, Flax.\n",
    "\n",
    "We have adopted the Gemma2 notebook from Google Deepmind to use HuggingFace's libraries and and simplified the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "import pdb\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.traverse_util import flatten_dict\n",
    "from flax.core.meta import unbox\n",
    "\n",
    "import optax\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "# Model partitioning related imports\n",
    "from jax.sharding import Mesh, NamedSharding\n",
    "from jax.sharding import PartitionSpec as PS\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "INPUT: Please provide your HUGGINGFACE_USERNAME:  felarof01\n",
      "INPUT: Please provide your HUGGINGFACE_TOKEN:  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\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace username and token to use when downloading.\n",
    "MODEL_NAME=\"felafax/gemma-2-2b-it-Flax\"\n",
    "HUGGINGFACE_USERNAME = input(\"INPUT: Please provide your HUGGINGFACE_USERNAME: \")\n",
    "HUGGINGFACE_TOKEN = input(\"INPUT: Please provide your HUGGINGFACE_TOKEN: \")\n",
    "\n",
    "model_name=MODEL_NAME\n",
    "hugging_face_token=HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "ckpt_path = snapshot_download(repo_id=MODEL_NAME, token=HUGGINGFACE_TOKEN)\n",
    "vocab_path = os.path.join(ckpt_path, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for key, value in flat_params.items():\n",
    "        name = \"/\".join(str(x) for x in key)\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Shape: {value.shape}\")\n",
    "        # print(f\"dtype: {value.dtype}\")\n",
    "        # print(f\"Value: {value}\")\n",
    "        if len(value.shape)<=2:\n",
    "            # jax.debug can't visualize higher dim params.\n",
    "            if isinstance(value, flax.core.meta.Partitioned):\n",
    "                array = unbox(value)\n",
    "            else:\n",
    "                array = value\n",
    "            print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: prepare the dataset\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=1, max_length=32, max_examples=32):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    def _tokenize(examples):\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length+1)\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        tokenized['target_mask'] = [input_id[:-1] for input_id in tokenized['attention_mask']]\n",
    "        return {\n",
    "            'input_tokens': tokenized['input_ids'],\n",
    "            'target_mask': tokenized['target_mask']\n",
    "        }\n",
    "\n",
    "    def _custom_collate_fn(batch):\n",
    "        \"\"\"Applies default_collate_fn from transformers and converts to JAX NumPy arrays.\"\"\"\n",
    "        batch = default_data_collator(batch)\n",
    "        jax_batch = {}\n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                jax_batch[key] = jnp.array(value.numpy())\n",
    "            else:\n",
    "                jax_batch[key] = value\n",
    "        \n",
    "        return jax_batch\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(max_examples))\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=_custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=_custom_collate_fn\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset loader by extracting a batch.\n",
    "# train_dataloader, _ = get_dataset(tokenizer=tokenizer)\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     if i>5:\n",
    "#         break\n",
    "#     input_ids, attention_mask = (\n",
    "#         batch[\"input_tokens\"],\n",
    "#         batch[\"target_mask\"],\n",
    "        \n",
    "#     )\n",
    "#     print(input_ids.shape)\n",
    "#     print()\n",
    "#     print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "id": "iEcV0XEEEcoZ"
   },
   "outputs": [],
   "source": [
    "def forward_and_loss_fn(params,\n",
    "                        *,\n",
    "                        state,\n",
    "                        input_tokens: jax.Array,            # Shape [B, L]\n",
    "                        input_mask: jax.Array,              # Shape [B, L]\n",
    "                        positions: jax.Array,               # Shape [B, L]\n",
    "                        attention_mask: jax.Array,          # [B, L, L]\n",
    "                        ) -> jax.Array:\n",
    "  \"\"\"Forward pass and loss function.\n",
    "\n",
    "  Args:\n",
    "    params: model's input parameters.\n",
    "    model: gemma transformer model to call.\n",
    "    input_tokens: input tokens sequence, shape [B, L].\n",
    "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
    "    positions: relative position of each token, shape [B, L].\n",
    "    attention_mask: input attention mask, shape [B, L].\n",
    "\n",
    "  Returns:\n",
    "    Softmax cross-entropy loss for the next-token prediction task.\n",
    "  \"\"\"\n",
    "\n",
    "  # Forward pass on the input data.\n",
    "  # No attention cache is needed here.\n",
    "  logits, _ = state.apply_fn(\n",
    "        {\"params\": params},\n",
    "        input_tokens,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "  # Exclude the last step as it does not appear in the targets.\n",
    "  logits = logits[:, :-1]\n",
    "\n",
    "  # Similarly, the first token cannot be predicteds.\n",
    "  target_tokens = input_tokens[:, 1:]\n",
    "  target_mask = input_mask[:, 1:]\n",
    "\n",
    "  # Convert the target labels into one-hot encoded vectors.\n",
    "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
    "\n",
    "  # Don't update on unwanted tokens.\n",
    "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[..., None]\n",
    "\n",
    "  # Normalisation factor.\n",
    "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
    "\n",
    "  # Return the nll loss.\n",
    "  loss =  -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor\n",
    "  # pdb.set_trace()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83DimpjEcoZ"
   },
   "source": [
    "The Gemma transformer requires an attention mask and position vector alongside each input. We can conveniently generate these using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "id": "cbWfdHf0EcoZ"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxYMMWLEcoZ"
   },
   "source": [
    "We can now build the train_step function which performs the backward pass and updates the model's parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellView": "form",
    "id": "cPSfp7ZUEcoZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_step(state,\n",
    "               batch,\n",
    "              pad_id: int,\n",
    "              ):\n",
    "  \"\"\"Train step.\n",
    "\n",
    "  Args:\n",
    "    model: gemma transformer model.\n",
    "    params: model's input parameters.\n",
    "    pad_id: id of the pad token.\n",
    "    batch: input batch.\n",
    "\n",
    "  Returns:\n",
    "    Training loss, updated parameters, updated optimizer state.\n",
    "  \"\"\"\n",
    "  # Build the position and attention mask vectors.\n",
    "  positions, attention_mask = get_attention_mask_and_positions(batch['input_tokens'], pad_id)\n",
    "\n",
    "  # pdb.set_trace()\n",
    "  # Forward and backward passes\n",
    "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(state.params,\n",
    "                                                             state=state,\n",
    "                                                             input_tokens=batch['input_tokens'],\n",
    "                                                             input_mask=batch['target_mask'],\n",
    "                                                             positions=positions,\n",
    "                                                             attention_mask=attention_mask)\n",
    "\n",
    "  # pdb.set_trace()\n",
    "  # Update the parameters\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return state, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2QXp116EcoZ"
   },
   "source": [
    "Similarly, we build a `validation_step` function without backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_params_pytree(params, mesh):\n",
    "    def shard_param(param):\n",
    "        if len(param.shape) == 0:\n",
    "            return NamedSharding(mesh, PS())\n",
    "        elif len(param.shape) == 1:\n",
    "            return NamedSharding(mesh, PS('model'))\n",
    "        elif len(param.shape) == 2:\n",
    "            return NamedSharding(mesh, PS('data', 'model'))\n",
    "        elif len(param.shape) == 3:\n",
    "            return NamedSharding(mesh, PS('data', 'model', 'replica'))\n",
    "        else:\n",
    "            # For higher-dimensional tensors, might need a more complex strategy. But reeplicate by default fornow.\n",
    "            return NamedSharding(mesh, PS())\n",
    "\n",
    "    return jax.tree_util.tree_map(shard_param, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  eval_every_n: int\n",
    "  batch_size: int\n",
    "  max_steps: int | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainstate_from_params(params, model_apply_fn, optimizer):\n",
    "    state = train_state.TrainState.create(\n",
    "        apply_fn=model_apply_fn,\n",
    "        params=params['params'],\n",
    "        tx=optimizer)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model: transformer_lib.Transformer,\n",
    "    params,\n",
    "    train_dataloader,\n",
    "    tokenizer,\n",
    "    training_cfg: TrainingConfig,\n",
    "    mesh):\n",
    "\n",
    "    # To create sharded train step, you need to figure out how params would look when sharded, for that\n",
    "    # first, trace (eval) create_trainstate_from_params to get param shapes.\n",
    "    # second, then shard the params with shapes.\n",
    "    # third, create sharded_train_step passing sharded param shapes as input to compiler/pjit via in_shardings.\n",
    "    #   out_shardings can be skipped in jax.jit.\n",
    "    \n",
    "    state_shapes = jax.eval_shape(\n",
    "        functools.partial(\n",
    "            create_trainstate_from_params,\n",
    "            params=params,\n",
    "            model_apply_fn=model.apply,\n",
    "            optimizer=optimizer,\n",
    "        ),\n",
    "    )\n",
    "    state_shapes_partitioned = shard_params_pytree(\n",
    "        state_shapes, mesh\n",
    "    )\n",
    "    sharded_train_step = jax.jit(\n",
    "        train_step,\n",
    "        in_shardings=(state_shapes_partitioned, NamedSharding(mesh, PS())),\n",
    "        out_shardings=(state_shapes_partitioned, NamedSharding(mesh, PS())),\n",
    "        static_argnums=(2,)\n",
    "        # donate_argnums=(0, 1),\n",
    "    )\n",
    "    \n",
    "    n_steps = 0\n",
    "    avg_loss=0\n",
    "\n",
    "    # here I'm first creating params which are unsharded. Then, during train_step, it should get sharded.\n",
    "    # if you visualize the params now, it shouldn't be sharded -- YEP, verified it, everything on TPU0.\n",
    "    state = create_trainstate_from_params(params, model.apply, optimizer)\n",
    "    \n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        train_batch = jax.device_put(train_batch, NamedSharding(mesh, PS()))\n",
    "        state, train_loss = sharded_train_step(state, train_batch, tokenizer.pad_token_id, )\n",
    "        n_steps += 1\n",
    "        avg_loss += train_loss\n",
    "        print(f\"train_loss {train_loss}\")\n",
    "        if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
    "          break\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4, 1))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model', 'replica'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters.\n",
    "params = {\"params\": params_lib.load_and_format_params(os.path.join(ckpt_path, 'gemma2-2b-it'))['transformer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=20,\n",
    "                              batch_size=1,\n",
    "                              max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config.\n",
    "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=30)\n",
    "model = transformer_lib.Transformer(config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "optimizer = optax.sgd(training_cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ff5e26c95845ec8ccd21f6083274a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef05d3b8c2f749e4a63ac18e4bf6c175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 3.5580010414123535\n",
      "train_loss 3.219733476638794\n",
      "train_loss 2.9344639778137207\n",
      "train_loss 2.6792898178100586\n",
      "train_loss 2.4685018062591553\n",
      "train_loss 2.2560482025146484\n",
      "train_loss 2.0904698371887207\n",
      "train_loss 1.9557868242263794\n",
      "train_loss 1.824852705001831\n",
      "train_loss 1.7282675504684448\n",
      "train_loss 1.6210218667984009\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_dataset(tokenizer=tokenizer)\n",
    "\n",
    "# with chex.fake_jit():\n",
    "new_state = train_loop(model=model,\n",
    "                    params=params,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    tokenizer=tokenizer,\n",
    "                    training_cfg=training_cfg, \n",
    "                   mesh = mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run single train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_shapes = jax.eval_shape(\n",
    "#     functools.partial(\n",
    "#         create_trainstate_from_params,\n",
    "#         params=params,\n",
    "#         model=model,\n",
    "#         optimizer=optimizer,\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_partitioned = shard_params_pytree(\n",
    "#     state_shapes, mesh\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharded_create_trainstate_from_params = jax.jit(\n",
    "#     create_trainstate_from_params,\n",
    "#     in_shardings=(state_partitioned.params, ),\n",
    "#     donate_argnums=(0, ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharded_train_step = jax.jit(\n",
    "#     train_step,\n",
    "#     in_shardings=(state_partitioned, state_partitioned.params, NamedSharding(mesh, PS()), NamedSharding(mesh, PS())),\n",
    "#     # donate_argnums=(0, 1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader, _ = get_dataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_batch = next(iter(train_dataloader))\n",
    "# sample_batch = jax.device_put(sample_batch, NamedSharding(mesh, PS()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, loss = sharded_train_step(state, state.params, tokenizer.pad_token_id, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_params(state.params)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
